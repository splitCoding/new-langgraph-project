from enum import Enum
from typing import List

from pydantic import BaseModel, Field

from src.review.cache import get_cache
from src.review.state.state import State

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI


class BestReviewCandidate(BaseModel):
    id: int
    score: int


class BestReviews(BaseModel):
    """ì„ íƒëœ ìµœì  ë¦¬ë·°ë“¤ì˜ ID ëª©ë¡ì„ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°"""
    candidates: List[BestReviewCandidate] = Field(description="ì£¼ì–´ì§„ ì´ˆì ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ëŒ€í‘œ ë¦¬ë·° 3ê°œì˜ ID, SCORE ëª©ë¡")


class LLMName(str, Enum):
    GPT_3_5_TURBO = "gpt-3.5-turbo"
    GPT_4_1_NANO = "gpt-4.1-nano"
    GPT_4_1_MINI = "gpt-4.1-mini"
    GPT_o_4_MINI = "o4-mini"


class LLMConfig:
    def __init__(self, model: str, temperature: float | None = 0):
        self.model = model
        self.temperature = temperature


# LLM ì„¤ì •ë“¤
LLM_CONFIGS = {
    LLMName.GPT_3_5_TURBO: LLMConfig("gpt-3.5-turbo", 0),
    LLMName.GPT_4_1_NANO: LLMConfig("gpt-4.1-nano", 0),
    LLMName.GPT_4_1_MINI: LLMConfig("gpt-4.1-mini", 0),
    LLMName.GPT_o_4_MINI: LLMConfig("o4-mini", None),  # temperature ì—†ìŒ
}


# 2. ì¡°íšŒëœ ë¦¬ë·°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë…¸ë“œ
def check_review_exist(state: State) -> bool:
    """[NODE] ë¦¬ë·°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë…¸ë“œ (ì˜ˆì‹œ)"""
    print("\n--- ë¦¬ë·° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (check_review_exist) ---")
    if not state['reviews'] or len(state['reviews']) == 0:
        print("-> ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False  # ë¹ˆ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    print("-> ë¦¬ë·°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
    return True


# 3. ì—¬ëŸ¬ê°œì˜ LLMìœ¼ë¡œ FAN-OUT í•˜ëŠ” ë…¸ë“œ
def use_llm_node(state: State) -> dict:
    print(f"\n--- BEST ë¦¬ë·° ìƒì„± ì‹œì‘ ---")
    return {}


# 4. LLM BEST ë¦¬ë·° í›„ë³´ ì„ ì • ë…¸ë“œ ìƒì„± í•¨ìˆ˜
def create_llm_selector_node(llm_name: LLMName, focus_instruction: str):
    """[NODE] LLMì´ ìµœì  ë¦¬ë·°ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë…¸ë“œ ìƒì„± í•¨ìˆ˜."""

    def llm_selector_node(state: State) -> dict:
        llm_config = LLM_CONFIGS[llm_name]
        print(f"\n--- 2. {llm_name}ìœ¼ë¡œ ë¦¬ë·° ì„ íƒ (ì´ˆì : {focus_instruction[:50]}...) ---")
        reviews = state.get("reviews", [])
        if not reviews:
            return {"selected_reviews": []}

        # ğŸ¯ ìºì‹œ í™•ì¸
        candidate_count = state.get('candidate_count', 3)
        cache = get_cache()
        cached_result = cache.get_cached_result(
            llm_name=llm_config.model,
            reviews=reviews,
            focus_instruction=focus_instruction,
            candidate_count=candidate_count
        )

        if cached_result is not None:
            print(f"âœ… ìºì‹œ HIT - ê²°ê³¼ ë°˜í™˜ (LLM í˜¸ì¶œ ìƒëµ)")
            return {"selected_reviews": cached_result}

        print(f"ğŸ”„ ìºì‹œ ë¯¸ìŠ¤ - LLM í˜¸ì¶œ ì§„í–‰")

        prompt = """
            ë‹¹ì‹ ì€ ê³ ê° ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¦¬ë·° ëª©ë¡ì—ì„œ íŠ¹ì • ì´ˆì ì— ë§ì¶° ê³ ê°ë“¤ì—ê²Œ ê°€ì¥ ìœ ìš©í•˜ê³  ëŒ€í‘œì ì¸ ë¦¬ë·°ë“¤ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
            ì„ íƒëœ ë¦¬ë·°ì˜ ID, ë¦¬ë·°ì˜ í‰ê°€ score ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
            í‰ê°€ score ëŠ” 0ì—ì„œ 100 ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ, ë¦¬ë·°ì˜ ìœ ìš©ì„±, ì‹ ë¢°ì„±, ì •ë³´ëŸ‰ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•œ ì ìˆ˜ì…ë‹ˆë‹¤.
            ë‹¹ì‹ ì˜ ë¶„ì„ ì´ˆì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {focus}

            {format_instructions}
        """
        parser = PydanticOutputParser(pydantic_object=BestReviews)
        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt),
            ("user", "ë‹¤ìŒì€ ê³ ê° ë¦¬ë·° ëª©ë¡ì…ë‹ˆë‹¤:\n\n{review_list}\n\nìœ„ ëª©ë¡ì—ì„œ ë‹¹ì‹ ì˜ ë¶„ì„ ì´ˆì ì— ë§ì¶° ìµœì ì˜ ë¦¬ë·° {candidate_count}ê°œë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        ])
        
        # temperatureê°€ Noneì¸ ê²½ìš° ChatOpenAIì— ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        model_kwargs = {"model": llm_config.model}
        if llm_config.temperature is not None:
            model_kwargs["temperature"] = llm_config.temperature
        
        model = ChatOpenAI(**model_kwargs)
        chain = prompt | model | parser

        review_list_str = "\n".join([
            f"- ID: {r['id']}, í‰ì : {r['rating']}, ë‚´ìš©: {r['text']}, ì´ë¯¸ì§€ ì¡´ì¬ ì—¬ë¶€ : {r['image_exist']}, ì‘ì„±ì¼ : {r['createdAt']}"
            for r in reviews
        ])

        try:
            response = chain.invoke({
                "focus": focus_instruction,
                "review_list": review_list_str,
                "candidate_count": candidate_count,
                "format_instructions": parser.get_format_instructions()
            })
            candidates = response.candidates
            print(f"-> {llm_name} ì„ íƒ ID: {[c.id for c in candidates]}")

            selected_reviews_map = {r['id']: r for r in reviews}
            final_selected_reviews = [
                {**selected_reviews_map[candidate.id], "score": candidate.score}
                for candidate in candidates if candidate.id in selected_reviews_map
            ]

            # ğŸ¯ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            cache.save_result(
                llm_name=llm_config.model,  # ìºì‹œ ì¡°íšŒì‹œì™€ ë™ì¼í•œ í‚¤ ì‚¬ìš©
                reviews=reviews,
                focus_instruction=focus_instruction,
                candidate_count=candidate_count,
                result=final_selected_reviews
            )

            # fan-inì„ ìœ„í•´ 'selected_reviews' í‚¤ë¡œ ë°˜í™˜
            return {"selected_reviews": final_selected_reviews}
        except Exception as e:
            print(f"LLM({llm_name}) í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"selected_reviews": []}

    return llm_selector_node


# ê°ê¸° ë‹¤ë¥¸ ì„ íƒì„ í•˜ëŠ” 3ê°œì˜ LLM ë…¸ë“œ ìƒì„±
focus_instruction = """
êµ¬ë§¤ë¥¼ ë§ì„¤ì´ëŠ” ê³ ê°ì—ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡,
ì œí’ˆì˜ ì¥ì ê³¼ ë‹¨ì ì„ ê· í˜• ìˆê²Œ ì „ë‹¬í•˜ë©°,
ì‚¬ì§„ì´ë‚˜ êµ¬ì²´ì ì¸ ì‚¬ìš© ì˜ˆì‹œê°€ í¬í•¨ë˜ì–´ ìˆê³ ,
ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ìƒì„¸í•œ ì„¤ëª…ê³¼ ì„±ëŠ¥ì— ëŒ€í•œ ê°ê´€ì  í‰ê°€ê°€ ë‹´ê²¨ ìˆìœ¼ë©°,
ë‚´ìš©ì´ ê¸¸ê³  ì¸ì‚¬ì´íŠ¸ê°€ í’ë¶€í•˜ê±°ë‚˜ ì§§ì§€ë§Œ ì¸ìƒì ì¸ í•µì‹¬ ë¬¸ì¥ì´ ìˆê³ ,
ì—¬ëŸ¬ ì‚¬ëŒì´ ê³µê°í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì  ì‚¬ìš© ê²½í—˜ê³¼ ìµœê·¼ ì‘ì„±ëœ ì‹ ë¢°ì„± ìˆëŠ” ê¸ì •ì  ë¦¬ë·°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•´ ì£¼ì„¸ìš”.
"""
llm1_select = create_llm_selector_node(LLMName.GPT_o_4_MINI, focus_instruction)
llm2_select = create_llm_selector_node(LLMName.GPT_4_1_NANO, focus_instruction)
llm3_select = create_llm_selector_node(LLMName.GPT_4_1_MINI, focus_instruction)

__all__ = [
    "check_review_exist",
    "use_llm_node",
    "llm1_select",
    "llm2_select",
    "llm3_select",
]
