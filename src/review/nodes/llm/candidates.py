from typing import List

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from src.review.cache import get_cache
from src.review.models import LLMModel, models
from src.review.prompts import prompts
from src.review.state.state import State


class BestReviewCandidate(BaseModel):
    id: int
    score: int


class BestReviews(BaseModel):
    """ì„ íƒëœ ìµœì  ë¦¬ë·°ë“¤ì˜ ID ëª©ë¡ì„ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°"""
    candidates: List[BestReviewCandidate] = Field(description="ì£¼ì–´ì§„ ì´ˆì ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ëŒ€í‘œ ë¦¬ë·° 3ê°œì˜ ID, SCORE ëª©ë¡")


# 2. ì¡°íšŒëœ ë¦¬ë·°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë…¸ë“œ
def check_review_exist(state: State) -> bool:
    """[NODE] ë¦¬ë·°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë…¸ë“œ (ì˜ˆì‹œ)"""
    print("\n--- ë¦¬ë·° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (check_review_exist) ---")
    if not state['reviews'] or len(state['reviews']) == 0:
        print("-> ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False  # ë¹ˆ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    print("-> ë¦¬ë·°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
    return True


# 3. ì—¬ëŸ¬ê°œì˜ LLMìœ¼ë¡œ FAN-OUT í•˜ëŠ” ë…¸ë“œ
def use_llm_node(state: State) -> dict:
    print(f"\n--- BEST ë¦¬ë·° ìƒì„± ì‹œì‘ ---")
    return {}


# 4. LLM BEST ë¦¬ë·° í›„ë³´ ì„ ì • ë…¸ë“œ ìƒì„± í•¨ìˆ˜
def create_llm_selector_node(llm_model: LLMModel, focus_instruction: str):
    """[NODE] LLMì´ ìµœì  ë¦¬ë·°ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë…¸ë“œ ìƒì„± í•¨ìˆ˜."""

    def llm_selector_node(state: State) -> dict:
        print(f"\n--- 2. {llm_model.name}ìœ¼ë¡œ ë¦¬ë·° ì„ íƒ (ì´ˆì : {focus_instruction[:50]}...) ---")
        reviews = state.get("reviews", [])
        if not reviews:
            return {"selected_reviews": []}

        # ğŸ¯ ìºì‹œ í™•ì¸
        candidate_count = state.get('candidate_count', 3)
        cache = get_cache()
        cached_result = cache.get_cached_result(
            llm_name=llm_model.name,
            reviews=reviews,
            focus_instruction=focus_instruction,
            candidate_count=candidate_count
        )

        if cached_result is not None:
            print(f"âœ… ìºì‹œ HIT - ê²°ê³¼ ë°˜í™˜ (LLM í˜¸ì¶œ ìƒëµ)")
            return {"selected_reviews": cached_result}

        print(f"ğŸ”„ ìºì‹œ ë¯¸ìŠ¤ - LLM í˜¸ì¶œ ì§„í–‰")

        parser = PydanticOutputParser(pydantic_object=BestReviews)
        prompt = ChatPromptTemplate.from_messages([
            ("system", prompts['candidate_select'].system),
            ("user", prompts['candidate_select'].user)
        ])

        # temperatureê°€ Noneì¸ ê²½ìš° ChatOpenAIì— ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        model_kwargs = {"model": llm_model.name}
        if llm_model.temperature is not None:
            model_kwargs["temperature"] = llm_model.temperature

        model = ChatOpenAI(**model_kwargs)
        chain = prompt | model | parser

        review_list_str = "\n".join([
            f"- ID: {r['id']}, í‰ì : {r['rating']}, ë‚´ìš©: {r['text']}, ì´ë¯¸ì§€ ì¡´ì¬ ì—¬ë¶€ : {r['image_exist']}, ì‘ì„±ì¼ : {r['createdAt']}"
            for r in reviews
        ])

        try:
            response = chain.invoke({
                "focus": focus_instruction,
                "review_list": review_list_str,
                "candidate_count": candidate_count,
                "format_instructions": parser.get_format_instructions()
            })
            candidates = response.candidates
            print(f"-> {llm_model.name} ì„ íƒ ID: {[c.id for c in candidates]}")

            selected_reviews_map = {r['id']: r for r in reviews}
            final_selected_reviews = [
                {**selected_reviews_map[candidate.id], "score": candidate.score}
                for candidate in candidates if candidate.id in selected_reviews_map
            ]

            # ğŸ¯ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            cache.save_result(
                llm_name=llm_model.name,  # ìºì‹œ ì¡°íšŒì‹œì™€ ë™ì¼í•œ í‚¤ ì‚¬ìš©
                reviews=reviews,
                focus_instruction=focus_instruction,
                candidate_count=candidate_count,
                result=final_selected_reviews
            )

            # fan-inì„ ìœ„í•´ 'selected_reviews' í‚¤ë¡œ ë°˜í™˜
            return {"selected_reviews": final_selected_reviews}
        except Exception as e:
            print(f"LLM({llm_model.name}) í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"selected_reviews": []}

    return llm_selector_node


# ê°ê¸° ë‹¤ë¥¸ ì„ íƒì„ í•˜ëŠ” 3ê°œì˜ LLM ë…¸ë“œ ìƒì„±
focus_instruction = """
êµ¬ë§¤ë¥¼ ë§ì„¤ì´ëŠ” ê³ ê°ì—ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡,
ì œí’ˆì˜ ì¥ì ê³¼ ë‹¨ì ì„ ê· í˜• ìˆê²Œ ì „ë‹¬í•˜ë©°,
ì‚¬ì§„ì´ë‚˜ êµ¬ì²´ì ì¸ ì‚¬ìš© ì˜ˆì‹œê°€ í¬í•¨ë˜ì–´ ìˆê³ ,
ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ìƒì„¸í•œ ì„¤ëª…ê³¼ ì„±ëŠ¥ì— ëŒ€í•œ ê°ê´€ì  í‰ê°€ê°€ ë‹´ê²¨ ìˆìœ¼ë©°,
ë‚´ìš©ì´ ê¸¸ê³  ì¸ì‚¬ì´íŠ¸ê°€ í’ë¶€í•˜ê±°ë‚˜ ì§§ì§€ë§Œ ì¸ìƒì ì¸ í•µì‹¬ ë¬¸ì¥ì´ ìˆê³ ,
ì—¬ëŸ¬ ì‚¬ëŒì´ ê³µê°í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì  ì‚¬ìš© ê²½í—˜ê³¼ ìµœê·¼ ì‘ì„±ëœ ì‹ ë¢°ì„± ìˆëŠ” ê¸ì •ì  ë¦¬ë·°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•´ ì£¼ì„¸ìš”.
"""
llm1_select = create_llm_selector_node(models['gpt_o_4_mini'], focus_instruction)
llm2_select = create_llm_selector_node(models['gpt_4_1_nano'], focus_instruction)
llm3_select = create_llm_selector_node(models['gpt_4_1_mini'], focus_instruction)

__all__ = [
    "check_review_exist",
    "use_llm_node",
    "llm1_select",
    "llm2_select",
    "llm3_select",
]
